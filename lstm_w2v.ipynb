{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24783, 8)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('preprocessed_data.csv')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, ..., 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=dataset['class'].values\n",
    "y\n",
    "## 0:hate speech; 1:offensive language; 2:neither"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            0\n",
       "count                 0\n",
       "hate_speech           0\n",
       "offensive_language    0\n",
       "neither               0\n",
       "class                 0\n",
       "tweet                 0\n",
       "processed_tweet       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(axis=0,how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24781, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk.data\n",
    "#nltk.download('punkt')\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tweets into lists of sentences where each sentence is a list of words\n",
    "#def tweet_tokenize(review, tokenizer, remove_sw=False):\n",
    "#    tokenized_tweets = tokenizer.tokenize(review)\n",
    "#    new_tweet = []\n",
    "#    for tokenized_tweet in tokenized_tweets:\n",
    "#        if len(tokenized_tweet) > 0:\n",
    "#            new_tweet.append(data_preprocess(tokenized_tweet, remove_sw))\n",
    "#    return new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_tweets = []\n",
    "#for review in dataset['tweet']:\n",
    "#    new_tweets += tweet_tokenize(review,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec model\n",
    "# simple tokenize\n",
    "texts = dataset['processed_tweet']\n",
    "tokenized_tweet = texts.apply(lambda x:str(x).split())\n",
    "\n",
    "from gensim.models import word2vec\n",
    "size = 200\n",
    "min_count = 2\n",
    "workers = 4\n",
    "window = 10\n",
    "sg = 1\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_tweet,\n",
    "                              vector_size = size,\n",
    "                              min_count = min_count,\n",
    "                              window = window,\n",
    "                              sg = 1,\n",
    "                              workers = workers)\n",
    "model_name = \"simplew2v\"\n",
    "w2v_model.save(model_name)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dress', 0.9455119967460632),\n",
       " ('way', 0.9453248977661133),\n",
       " ('nice', 0.9434027671813965),\n",
       " ('truth', 0.9430713057518005),\n",
       " ('repli', 0.941224217414856),\n",
       " ('drive', 0.9405822157859802),\n",
       " ('chick', 0.9402672052383423),\n",
       " ('fun', 0.9395648837089539),\n",
       " ('asshol', 0.9383838772773743),\n",
       " ('enough', 0.9382278919219971)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most similar words present in the model\n",
    "w2v_model.wv.most_similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.18818495, -0.16028605,  0.0977423 , -0.01033437,  0.03547648,\n",
       "       -0.19841255,  0.16995348,  0.23270577, -0.12913476, -0.06824369,\n",
       "        0.02428589, -0.19027972, -0.09466037,  0.15572235, -0.19010308,\n",
       "        0.06238107,  0.01550262,  0.11662454, -0.09587722, -0.33045298,\n",
       "        0.32646105, -0.07574545,  0.20194137,  0.06717928, -0.05720104,\n",
       "        0.01044636, -0.07381954, -0.04111833, -0.10158026,  0.03280369,\n",
       "        0.17351843,  0.05731299,  0.16931362,  0.13573097,  0.10728898,\n",
       "       -0.18229297,  0.24536929, -0.21144088, -0.18908644, -0.19703043,\n",
       "       -0.08628925,  0.00425185, -0.12827305,  0.1360265 ,  0.12387665,\n",
       "        0.07816706, -0.0078771 , -0.00748067,  0.01100683, -0.0096606 ,\n",
       "       -0.18109472, -0.23633201, -0.14294688, -0.03657249,  0.16448505,\n",
       "       -0.04117519, -0.1045759 , -0.12082563, -0.29745558,  0.05899406,\n",
       "       -0.00955477,  0.02441569,  0.08238562,  0.11793819, -0.39334965,\n",
       "        0.19249637,  0.05157324,  0.42357603, -0.29402754,  0.35676992,\n",
       "       -0.03639227,  0.1177535 ,  0.25022328,  0.08552928,  0.04098374,\n",
       "       -0.091394  ,  0.3242565 , -0.16450082, -0.3783979 ,  0.00717287,\n",
       "       -0.00300183,  0.01407854, -0.3399103 ,  0.24540135, -0.26326162,\n",
       "       -0.08976192,  0.06614395, -0.00607146, -0.11236891,  0.06313433,\n",
       "        0.18714212,  0.07898874,  0.2533941 ,  0.18373846,  0.45621768,\n",
       "        0.15071847, -0.1063509 , -0.07908481,  0.1714336 ,  0.00428148,\n",
       "        0.00628483,  0.24524035,  0.18316257, -0.02570241, -0.08314927,\n",
       "       -0.16415115, -0.00178745,  0.33166546, -0.27600166, -0.2495472 ,\n",
       "       -0.11086652, -0.18331034, -0.10048442,  0.03570547,  0.25663367,\n",
       "       -0.13283496,  0.14493893, -0.14041714,  0.00794851, -0.34279838,\n",
       "        0.08431585,  0.06676884, -0.03014368, -0.32499447, -0.12230581,\n",
       "        0.01966776, -0.3949003 , -0.13179326,  0.10660233,  0.14737628,\n",
       "       -0.046306  , -0.00181929, -0.08993335, -0.24720176, -0.12628658,\n",
       "        0.31477362,  0.00514665, -0.04874741,  0.05513141, -0.2604378 ,\n",
       "        0.23164497, -0.42068583, -0.00638014,  0.02923501,  0.08149108,\n",
       "       -0.10164702, -0.12119908,  0.15183628,  0.09621739,  0.15508136,\n",
       "        0.11107881, -0.02865834,  0.08077019,  0.25139976, -0.16727613,\n",
       "        0.31800818,  0.2320986 ,  0.12396548,  0.12053274,  0.12842244,\n",
       "        0.0688232 , -0.01358259, -0.10167838, -0.1259924 ,  0.02378592,\n",
       "        0.13011268,  0.06747407, -0.00379846, -0.18200408,  0.00796194,\n",
       "       -0.13931769,  0.00621474,  0.14892186,  0.13326457,  0.07668372,\n",
       "        0.18736471, -0.06919421,  0.24858984,  0.06736457, -0.06681848,\n",
       "       -0.02101227,  0.14832336, -0.07857396,  0.00548763,  0.15972877,\n",
       "        0.12070449, -0.03035485,  0.03421916,  0.25611553, -0.03125391,\n",
       "        0.09948811,  0.03027971, -0.2056049 , -0.15216348,  0.04516792,\n",
       "        0.0498763 ,  0.013386  ,  0.00348219,  0.15036762, -0.03368822],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the vector representation for any word from our corpus\n",
    "w2v_model.wv.get_vector('shit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85078007"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the similarity of two words\n",
    "w2v_model.wv.similarity('man','dude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19240057,  0.11275952,  0.11498567, ..., -0.15782012,\n",
       "        -0.09665237,  0.27106634],\n",
       "       [ 0.22625284,  0.08221275, -0.0785579 , ..., -0.1078688 ,\n",
       "         0.0409813 ,  0.1591981 ],\n",
       "       [ 0.05883907, -0.12780347, -0.09040714, ..., -0.06191109,\n",
       "         0.07760243, -0.09103731],\n",
       "       ...,\n",
       "       [ 0.03057816, -0.00221626,  0.03972917, ..., -0.01921675,\n",
       "         0.00532114, -0.0220346 ],\n",
       "       [ 0.03632252,  0.00739879,  0.04732922, ..., -0.02646327,\n",
       "        -0.0070594 , -0.02994229],\n",
       "       [ 0.02892422,  0.01274518,  0.04523262, ..., -0.02520522,\n",
       "         0.00271638, -0.03352645]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve the weights from the model\n",
    "w2v_weights = w2v_model.wv.vectors\n",
    "w2v_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size, embedding_size = w2v_weights.shape\n",
    "#print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7470"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = w2v_model.wv.key_to_index.keys()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no of key-value pairs :  7470\n"
     ]
    }
   ],
   "source": [
    "word_vec_dict = {}\n",
    "for word in vocab:\n",
    "    word_vec_dict[word] = w2v_model.wv.get_vector(word)\n",
    "print(\"The no of key-value pairs : \",len(word_vec_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max length of tweets dataset['processed_tweet']\n",
    "maxlen = -1\n",
    "for i, rev in enumerate(dataset['processed_tweet']):\n",
    "    tweet = str(rev).split()\n",
    "    if (len(tweet)>maxlen):\n",
    "        maxlen = len(tweet)\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_tweet)\n",
    "X = tokenizer.texts_to_sequences(tokenized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24781, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, padding='pre',maxlen=28)\n",
    "X.shape # 24781 tweets, and have padded each tweet to be of max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.19240057,  0.11275952,  0.11498567, ..., -0.15782012,\n",
       "        -0.09665237,  0.27106634],\n",
       "       [ 0.22625284,  0.08221275, -0.0785579 , ..., -0.1078688 ,\n",
       "         0.0409813 ,  0.15919811],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "w_matrix = np.zeros((vocab_size, size))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedd_vector = word_vec_dict.get(word)\n",
    "    if embedd_vector is not None:\n",
    "        w_matrix[i] = embedd_vector\n",
    "\n",
    "w_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dropout, Dense, LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = size, input_length = maxlen, embeddings_initializer=Constant(w_matrix))) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 28, 200)           3095400   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 28, 200)           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                67840     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,171,625\n",
      "Trainable params: 3,171,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24781,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=dataset['class'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "697/697 [==============================] - 29s 39ms/step - loss: 0.1621 - accuracy: 0.7674 - val_loss: 0.0967 - val_accuracy: 0.7729\n",
      "Epoch 2/50\n",
      "697/697 [==============================] - 29s 42ms/step - loss: 0.0920 - accuracy: 0.7759 - val_loss: 0.0892 - val_accuracy: 0.7749\n",
      "Epoch 3/50\n",
      "697/697 [==============================] - 27s 39ms/step - loss: 0.0709 - accuracy: 0.7846 - val_loss: 0.0909 - val_accuracy: 0.7729\n",
      "Epoch 4/50\n",
      "697/697 [==============================] - 28s 40ms/step - loss: 0.0548 - accuracy: 0.7954 - val_loss: 0.0972 - val_accuracy: 0.7717\n",
      "Epoch 5/50\n",
      "697/697 [==============================] - 29s 42ms/step - loss: 0.0437 - accuracy: 0.8034 - val_loss: 0.1027 - val_accuracy: 0.7656\n",
      "Epoch 6/50\n",
      "697/697 [==============================] - 31s 44ms/step - loss: 0.0376 - accuracy: 0.8089 - val_loss: 0.1038 - val_accuracy: 0.7668\n",
      "Epoch 7/50\n",
      "697/697 [==============================] - 31s 45ms/step - loss: 0.0332 - accuracy: 0.8120 - val_loss: 0.1096 - val_accuracy: 0.7620\n",
      "Epoch 8/50\n",
      "697/697 [==============================] - 32s 46ms/step - loss: 0.0292 - accuracy: 0.8138 - val_loss: 0.1137 - val_accuracy: 0.7616\n",
      "Epoch 9/50\n",
      "697/697 [==============================] - 38s 54ms/step - loss: 0.0272 - accuracy: 0.8158 - val_loss: 0.1072 - val_accuracy: 0.7648\n",
      "Epoch 10/50\n",
      "697/697 [==============================] - 34s 48ms/step - loss: 0.0254 - accuracy: 0.8163 - val_loss: 0.1177 - val_accuracy: 0.7624\n",
      "Epoch 11/50\n",
      "697/697 [==============================] - 33s 48ms/step - loss: 0.0244 - accuracy: 0.8170 - val_loss: 0.1067 - val_accuracy: 0.7693\n",
      "Epoch 12/50\n",
      "697/697 [==============================] - 32s 46ms/step - loss: 0.0218 - accuracy: 0.8186 - val_loss: 0.1158 - val_accuracy: 0.7592\n",
      "Epoch 13/50\n",
      "697/697 [==============================] - 32s 46ms/step - loss: 0.0209 - accuracy: 0.8188 - val_loss: 0.1192 - val_accuracy: 0.7535\n",
      "Epoch 14/50\n",
      "697/697 [==============================] - 33s 48ms/step - loss: 0.0200 - accuracy: 0.8186 - val_loss: 0.1097 - val_accuracy: 0.7717\n",
      "Epoch 15/50\n",
      "697/697 [==============================] - 31s 45ms/step - loss: 0.0189 - accuracy: 0.8197 - val_loss: 0.1146 - val_accuracy: 0.7644\n",
      "Epoch 16/50\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0178 - accuracy: 0.8208 - val_loss: 0.1184 - val_accuracy: 0.7660\n",
      "Epoch 17/50\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0177 - accuracy: 0.8204 - val_loss: 0.1216 - val_accuracy: 0.7648\n",
      "Epoch 18/50\n",
      "697/697 [==============================] - 31s 44ms/step - loss: 0.0168 - accuracy: 0.8210 - val_loss: 0.1236 - val_accuracy: 0.7600\n",
      "Epoch 19/50\n",
      "697/697 [==============================] - 31s 45ms/step - loss: 0.0163 - accuracy: 0.8209 - val_loss: 0.1226 - val_accuracy: 0.7596\n",
      "Epoch 20/50\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0158 - accuracy: 0.8215 - val_loss: 0.1199 - val_accuracy: 0.7624\n",
      "Epoch 21/50\n",
      "697/697 [==============================] - 32s 45ms/step - loss: 0.0161 - accuracy: 0.8206 - val_loss: 0.1182 - val_accuracy: 0.7636\n",
      "Epoch 22/50\n",
      "697/697 [==============================] - 34s 49ms/step - loss: 0.0159 - accuracy: 0.8211 - val_loss: 0.1239 - val_accuracy: 0.7580\n",
      "Epoch 23/50\n",
      "697/697 [==============================] - 34s 49ms/step - loss: 0.0161 - accuracy: 0.8217 - val_loss: 0.1267 - val_accuracy: 0.7628\n",
      "Epoch 24/50\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0155 - accuracy: 0.8216 - val_loss: 0.1285 - val_accuracy: 0.7576\n",
      "Epoch 25/50\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0147 - accuracy: 0.8223 - val_loss: 0.1275 - val_accuracy: 0.7584\n",
      "Epoch 26/50\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0141 - accuracy: 0.8229 - val_loss: 0.1262 - val_accuracy: 0.7628\n",
      "Epoch 27/50\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0148 - accuracy: 0.8221 - val_loss: 0.1294 - val_accuracy: 0.7604\n",
      "Epoch 28/50\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0148 - accuracy: 0.8223 - val_loss: 0.1286 - val_accuracy: 0.7535\n",
      "Epoch 29/50\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0147 - accuracy: 0.8223 - val_loss: 0.1346 - val_accuracy: 0.7467\n",
      "Epoch 30/50\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0146 - accuracy: 0.8226 - val_loss: 0.1304 - val_accuracy: 0.7519\n",
      "Epoch 31/50\n",
      "697/697 [==============================] - 31s 44ms/step - loss: 0.0143 - accuracy: 0.8221 - val_loss: 0.1316 - val_accuracy: 0.7543\n",
      "Epoch 32/50\n",
      "697/697 [==============================] - 31s 44ms/step - loss: 0.0139 - accuracy: 0.8224 - val_loss: 0.1288 - val_accuracy: 0.7600\n",
      "Epoch 33/50\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0142 - accuracy: 0.8228 - val_loss: 0.1435 - val_accuracy: 0.7515\n",
      "Epoch 34/50\n",
      "697/697 [==============================] - 30s 44ms/step - loss: 0.0135 - accuracy: 0.8232 - val_loss: 0.1235 - val_accuracy: 0.7580\n",
      "Epoch 35/50\n",
      "697/697 [==============================] - 30s 44ms/step - loss: 0.0138 - accuracy: 0.8224 - val_loss: 0.1310 - val_accuracy: 0.7596\n",
      "Epoch 36/50\n",
      "697/697 [==============================] - 30s 44ms/step - loss: 0.0135 - accuracy: 0.8231 - val_loss: 0.1318 - val_accuracy: 0.7580\n",
      "Epoch 37/50\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0132 - accuracy: 0.8237 - val_loss: 0.1328 - val_accuracy: 0.7584\n",
      "Epoch 38/50\n",
      "697/697 [==============================] - 32s 45ms/step - loss: 0.0139 - accuracy: 0.8228 - val_loss: 0.1411 - val_accuracy: 0.7531\n",
      "Epoch 39/50\n",
      "697/697 [==============================] - 32s 46ms/step - loss: 0.0131 - accuracy: 0.8230 - val_loss: 0.1418 - val_accuracy: 0.7511\n",
      "Epoch 40/50\n",
      "697/697 [==============================] - 32s 45ms/step - loss: 0.0133 - accuracy: 0.8235 - val_loss: 0.1293 - val_accuracy: 0.7576\n",
      "Epoch 41/50\n",
      "697/697 [==============================] - 33s 47ms/step - loss: 0.0132 - accuracy: 0.8228 - val_loss: 0.1433 - val_accuracy: 0.7515\n",
      "Epoch 42/50\n",
      "697/697 [==============================] - 31s 45ms/step - loss: 0.0130 - accuracy: 0.8232 - val_loss: 0.1404 - val_accuracy: 0.7576\n",
      "Epoch 43/50\n",
      "697/697 [==============================] - 32s 45ms/step - loss: 0.0126 - accuracy: 0.8237 - val_loss: 0.1405 - val_accuracy: 0.7559\n",
      "Epoch 44/50\n",
      "697/697 [==============================] - 32s 45ms/step - loss: 0.0139 - accuracy: 0.8231 - val_loss: 0.1293 - val_accuracy: 0.7668\n",
      "Epoch 45/50\n",
      "697/697 [==============================] - 32s 45ms/step - loss: 0.0134 - accuracy: 0.8229 - val_loss: 0.1297 - val_accuracy: 0.7588\n",
      "Epoch 46/50\n",
      "697/697 [==============================] - 32s 46ms/step - loss: 0.0133 - accuracy: 0.8234 - val_loss: 0.1396 - val_accuracy: 0.7539\n",
      "Epoch 47/50\n",
      "697/697 [==============================] - 32s 46ms/step - loss: 0.0127 - accuracy: 0.8233 - val_loss: 0.1356 - val_accuracy: 0.7584\n",
      "Epoch 48/50\n",
      "697/697 [==============================] - 32s 46ms/step - loss: 0.0131 - accuracy: 0.8234 - val_loss: 0.1301 - val_accuracy: 0.7628\n",
      "Epoch 49/50\n",
      "697/697 [==============================] - 32s 45ms/step - loss: 0.0132 - accuracy: 0.8238 - val_loss: 0.1341 - val_accuracy: 0.7523\n",
      "Epoch 50/50\n",
      "697/697 [==============================] - 32s 46ms/step - loss: 0.0138 - accuracy: 0.8234 - val_loss: 0.1348 - val_accuracy: 0.7527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=0)\n",
    "hist = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = epochs,\n",
    "                 batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
