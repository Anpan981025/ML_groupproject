{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a64ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a35c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24783, 8)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('preprocessed_data.csv')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e7c41",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225cdd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            0\n",
       "count                 0\n",
       "hate_speech           0\n",
       "offensive_language    0\n",
       "neither               0\n",
       "class                 0\n",
       "tweet                 0\n",
       "processed_tweet       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97a7000",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(axis=0,how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce3fb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24781, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f4126",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc00db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(dataset[\"processed_tweet\"].apply(lambda x: str(x).split(\" \")))]\n",
    "\n",
    "# training of the model\n",
    "doc2vec_model = Doc2Vec(documents,vector_size=100, window=2, min_count=1, workers=4)\n",
    "\n",
    "# transform each document (tweet) into a vector data\n",
    "doc2vec_features = dataset[\"processed_tweet\"].apply(lambda x: doc2vec_model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
    "doc2vec_features.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_features.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a399d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc2vec_vector_0</th>\n",
       "      <th>doc2vec_vector_1</th>\n",
       "      <th>doc2vec_vector_2</th>\n",
       "      <th>doc2vec_vector_3</th>\n",
       "      <th>doc2vec_vector_4</th>\n",
       "      <th>doc2vec_vector_5</th>\n",
       "      <th>doc2vec_vector_6</th>\n",
       "      <th>doc2vec_vector_7</th>\n",
       "      <th>doc2vec_vector_8</th>\n",
       "      <th>doc2vec_vector_9</th>\n",
       "      <th>...</th>\n",
       "      <th>doc2vec_vector_90</th>\n",
       "      <th>doc2vec_vector_91</th>\n",
       "      <th>doc2vec_vector_92</th>\n",
       "      <th>doc2vec_vector_93</th>\n",
       "      <th>doc2vec_vector_94</th>\n",
       "      <th>doc2vec_vector_95</th>\n",
       "      <th>doc2vec_vector_96</th>\n",
       "      <th>doc2vec_vector_97</th>\n",
       "      <th>doc2vec_vector_98</th>\n",
       "      <th>doc2vec_vector_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007499</td>\n",
       "      <td>-0.003608</td>\n",
       "      <td>-0.001642</td>\n",
       "      <td>0.025057</td>\n",
       "      <td>-0.018753</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.012677</td>\n",
       "      <td>0.018329</td>\n",
       "      <td>-0.031626</td>\n",
       "      <td>0.010353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>-0.002859</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.006959</td>\n",
       "      <td>0.027266</td>\n",
       "      <td>-0.018546</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>0.027791</td>\n",
       "      <td>-0.015647</td>\n",
       "      <td>-0.005814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.029234</td>\n",
       "      <td>0.013032</td>\n",
       "      <td>0.028907</td>\n",
       "      <td>0.009010</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>-0.044108</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>0.065313</td>\n",
       "      <td>-0.026325</td>\n",
       "      <td>-0.000356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040774</td>\n",
       "      <td>0.028754</td>\n",
       "      <td>0.028741</td>\n",
       "      <td>-0.014615</td>\n",
       "      <td>0.062676</td>\n",
       "      <td>0.018202</td>\n",
       "      <td>-0.003816</td>\n",
       "      <td>-0.038756</td>\n",
       "      <td>0.029557</td>\n",
       "      <td>0.002088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.020330</td>\n",
       "      <td>0.011608</td>\n",
       "      <td>-0.005730</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>-0.006354</td>\n",
       "      <td>-0.003547</td>\n",
       "      <td>0.016893</td>\n",
       "      <td>-0.012370</td>\n",
       "      <td>-0.011603</td>\n",
       "      <td>-0.016983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001925</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>-0.014032</td>\n",
       "      <td>0.017906</td>\n",
       "      <td>-0.017390</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.004429</td>\n",
       "      <td>-0.000281</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>0.009672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.011350</td>\n",
       "      <td>-0.005278</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.027390</td>\n",
       "      <td>-0.013552</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.013267</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>-0.005738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.016836</td>\n",
       "      <td>0.024767</td>\n",
       "      <td>-0.009926</td>\n",
       "      <td>0.025036</td>\n",
       "      <td>0.018513</td>\n",
       "      <td>0.009169</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>-0.017305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.020420</td>\n",
       "      <td>0.017002</td>\n",
       "      <td>0.011147</td>\n",
       "      <td>-0.009374</td>\n",
       "      <td>-0.012437</td>\n",
       "      <td>-0.022879</td>\n",
       "      <td>0.020928</td>\n",
       "      <td>0.027851</td>\n",
       "      <td>-0.013634</td>\n",
       "      <td>-0.008545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021988</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>-0.014586</td>\n",
       "      <td>0.008590</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.014557</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>-0.018876</td>\n",
       "      <td>0.003080</td>\n",
       "      <td>0.011958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>0.010484</td>\n",
       "      <td>0.024478</td>\n",
       "      <td>0.039817</td>\n",
       "      <td>0.013727</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>-0.069086</td>\n",
       "      <td>-0.002619</td>\n",
       "      <td>0.136675</td>\n",
       "      <td>-0.063903</td>\n",
       "      <td>0.018723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065459</td>\n",
       "      <td>0.012917</td>\n",
       "      <td>0.029792</td>\n",
       "      <td>-0.026898</td>\n",
       "      <td>0.138854</td>\n",
       "      <td>0.021354</td>\n",
       "      <td>0.022084</td>\n",
       "      <td>-0.040775</td>\n",
       "      <td>0.012494</td>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>-0.025994</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>0.025043</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>-0.019190</td>\n",
       "      <td>-0.030451</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>0.034775</td>\n",
       "      <td>-0.026322</td>\n",
       "      <td>-0.008368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024668</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.031659</td>\n",
       "      <td>0.017184</td>\n",
       "      <td>-0.002900</td>\n",
       "      <td>-0.017701</td>\n",
       "      <td>-0.002657</td>\n",
       "      <td>0.012225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>-0.070315</td>\n",
       "      <td>0.021987</td>\n",
       "      <td>-0.001204</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.020636</td>\n",
       "      <td>0.008880</td>\n",
       "      <td>0.059381</td>\n",
       "      <td>-0.015007</td>\n",
       "      <td>0.007781</td>\n",
       "      <td>-0.036368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052554</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>-0.016332</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>-0.025229</td>\n",
       "      <td>0.026032</td>\n",
       "      <td>-0.003911</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.008153</td>\n",
       "      <td>-0.021806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>-0.001312</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>0.024085</td>\n",
       "      <td>-0.008341</td>\n",
       "      <td>-0.006602</td>\n",
       "      <td>-0.022620</td>\n",
       "      <td>-0.004195</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>-0.017632</td>\n",
       "      <td>-0.007274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024818</td>\n",
       "      <td>0.016793</td>\n",
       "      <td>0.005234</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>0.030537</td>\n",
       "      <td>0.012518</td>\n",
       "      <td>0.011066</td>\n",
       "      <td>-0.028579</td>\n",
       "      <td>0.009684</td>\n",
       "      <td>0.012363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>0.060671</td>\n",
       "      <td>0.028226</td>\n",
       "      <td>0.085275</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>-0.121321</td>\n",
       "      <td>-0.057725</td>\n",
       "      <td>0.228531</td>\n",
       "      <td>-0.070937</td>\n",
       "      <td>0.047898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083998</td>\n",
       "      <td>0.049165</td>\n",
       "      <td>0.092679</td>\n",
       "      <td>-0.067476</td>\n",
       "      <td>0.256899</td>\n",
       "      <td>0.013173</td>\n",
       "      <td>0.012131</td>\n",
       "      <td>-0.078709</td>\n",
       "      <td>0.032178</td>\n",
       "      <td>-0.011523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24781 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc2vec_vector_0  doc2vec_vector_1  doc2vec_vector_2  doc2vec_vector_3  \\\n",
       "0              0.007499         -0.003608         -0.001642          0.025057   \n",
       "1             -0.029234          0.013032          0.028907          0.009010   \n",
       "2             -0.020330          0.011608         -0.005730          0.000129   \n",
       "3              0.003395          0.011350         -0.005278          0.000222   \n",
       "4             -0.020420          0.017002          0.011147         -0.009374   \n",
       "...                 ...               ...               ...               ...   \n",
       "24778          0.010484          0.024478          0.039817          0.013727   \n",
       "24779         -0.025994          0.012341          0.025043          0.000238   \n",
       "24780         -0.070315          0.021987         -0.001204          0.000615   \n",
       "24781         -0.001312          0.009120          0.024085         -0.008341   \n",
       "24782          0.060671          0.028226          0.085275         -0.002667   \n",
       "\n",
       "       doc2vec_vector_4  doc2vec_vector_5  doc2vec_vector_6  doc2vec_vector_7  \\\n",
       "0             -0.018753          0.007608          0.012677          0.018329   \n",
       "1              0.001612         -0.044108          0.013316          0.065313   \n",
       "2             -0.006354         -0.003547          0.016893         -0.012370   \n",
       "3              0.027390         -0.013552          0.003472          0.013267   \n",
       "4             -0.012437         -0.022879          0.020928          0.027851   \n",
       "...                 ...               ...               ...               ...   \n",
       "24778          0.008760         -0.069086         -0.002619          0.136675   \n",
       "24779         -0.019190         -0.030451          0.008782          0.034775   \n",
       "24780          0.020636          0.008880          0.059381         -0.015007   \n",
       "24781         -0.006602         -0.022620         -0.004195          0.033900   \n",
       "24782          0.028829         -0.121321         -0.057725          0.228531   \n",
       "\n",
       "       doc2vec_vector_8  doc2vec_vector_9  ...  doc2vec_vector_90  \\\n",
       "0             -0.031626          0.010353  ...           0.001433   \n",
       "1             -0.026325         -0.000356  ...           0.040774   \n",
       "2             -0.011603         -0.016983  ...          -0.001925   \n",
       "3              0.007195         -0.005738  ...           0.008400   \n",
       "4             -0.013634         -0.008545  ...           0.021988   \n",
       "...                 ...               ...  ...                ...   \n",
       "24778         -0.063903          0.018723  ...           0.065459   \n",
       "24779         -0.026322         -0.008368  ...           0.024668   \n",
       "24780          0.007781         -0.036368  ...           0.052554   \n",
       "24781         -0.017632         -0.007274  ...           0.024818   \n",
       "24782         -0.070937          0.047898  ...           0.083998   \n",
       "\n",
       "       doc2vec_vector_91  doc2vec_vector_92  doc2vec_vector_93  \\\n",
       "0              -0.002859          -0.000114          -0.006959   \n",
       "1               0.028754           0.028741          -0.014615   \n",
       "2               0.008584          -0.014032           0.017906   \n",
       "3               0.016836           0.024767          -0.009926   \n",
       "4               0.005240          -0.014586           0.008590   \n",
       "...                  ...                ...                ...   \n",
       "24778           0.012917           0.029792          -0.026898   \n",
       "24779           0.010854           0.000534           0.002048   \n",
       "24780           0.004320          -0.016332           0.004185   \n",
       "24781           0.016793           0.005234           0.006163   \n",
       "24782           0.049165           0.092679          -0.067476   \n",
       "\n",
       "       doc2vec_vector_94  doc2vec_vector_95  doc2vec_vector_96  \\\n",
       "0               0.027266          -0.018546           0.014691   \n",
       "1               0.062676           0.018202          -0.003816   \n",
       "2              -0.017390           0.009108           0.004429   \n",
       "3               0.025036           0.018513           0.009169   \n",
       "4               0.009662           0.014557           0.002863   \n",
       "...                  ...                ...                ...   \n",
       "24778           0.138854           0.021354           0.022084   \n",
       "24779           0.031659           0.017184          -0.002900   \n",
       "24780          -0.025229           0.026032          -0.003911   \n",
       "24781           0.030537           0.012518           0.011066   \n",
       "24782           0.256899           0.013173           0.012131   \n",
       "\n",
       "       doc2vec_vector_97  doc2vec_vector_98  doc2vec_vector_99  \n",
       "0               0.027791          -0.015647          -0.005814  \n",
       "1              -0.038756           0.029557           0.002088  \n",
       "2              -0.000281           0.005516           0.009672  \n",
       "3               0.001015           0.010097          -0.017305  \n",
       "4              -0.018876           0.003080           0.011958  \n",
       "...                  ...                ...                ...  \n",
       "24778          -0.040775           0.012494           0.001791  \n",
       "24779          -0.017701          -0.002657           0.012225  \n",
       "24780           0.002419           0.008153          -0.021806  \n",
       "24781          -0.028579           0.009684           0.012363  \n",
       "24782          -0.078709           0.032178          -0.011523  \n",
       "\n",
       "[24781 rows x 100 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a16dc219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max length of tweets dataset['processed_tweet']\n",
    "maxlen = -1\n",
    "for i, rev in enumerate(dataset['processed_tweet']):\n",
    "    tweet = str(rev).split()\n",
    "    if (len(tweet)>maxlen):\n",
    "        maxlen = len(tweet)\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc2fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet = dataset['processed_tweet'].apply(lambda x:str(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a69ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_tweet)\n",
    "X = tokenizer.texts_to_sequences(tokenized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0393592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24781, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, padding='pre',maxlen=28)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "577f239e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15476"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = doc2vec_model.wv.key_to_index.keys()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7293f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_dict = {}\n",
    "for word in vocab:\n",
    "    word_vec_dict[word] = doc2vec_model.wv.get_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a685b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-1.02485037e+00,  7.47610569e-01,  7.57614136e-01, ...,\n",
       "        -6.61297619e-01,  2.10512787e-01,  1.46599621e-01],\n",
       "       [-9.28829312e-01,  6.89395010e-01,  7.11957693e-01, ...,\n",
       "        -5.32279611e-01, -6.92608431e-02,  3.35961163e-01],\n",
       "       ...,\n",
       "       [ 8.38220178e-04,  1.18158972e-02,  2.62711029e-02, ...,\n",
       "        -2.92869899e-02,  2.08479371e-02,  2.62835016e-03],\n",
       "       [-2.78962310e-03,  1.99453868e-02,  3.71178575e-02, ...,\n",
       "        -3.75461020e-02,  1.76950395e-02,  7.41720106e-03],\n",
       "       [ 3.57250031e-03, -1.12868845e-03,  1.33451689e-02, ...,\n",
       "         5.45247179e-03,  2.04785727e-03, -1.34387822e-03]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "w_matrix = np.zeros((vocab_size, 100))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedd_vector = word_vec_dict.get(word)\n",
    "    if embedd_vector is not None:\n",
    "        w_matrix[i] = embedd_vector\n",
    "\n",
    "w_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d3dba",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "036ef129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dropout, Dense, LSTM, Embedding, Activation, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import Constant\n",
    "from keras.layers.convolutional import MaxPooling1D, Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b0e58d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = 100, input_length = maxlen, embeddings_initializer=Constant(w_matrix))) \n",
    "model.add(Dropout(0.2)) #0.2\n",
    "\n",
    "model.add(Bidirectional(LSTM(64))) #64\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40185611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 28, 100)           1547700   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 28, 100)           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              84480     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,644,661\n",
      "Trainable params: 1,644,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd430be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24781,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=dataset['class'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bdc550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d315bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "697/697 [==============================] - 23s 29ms/step - loss: 0.1837 - accuracy: 0.7682 - val_loss: 0.1083 - val_accuracy: 0.7729\n",
      "Epoch 2/50\n",
      "697/697 [==============================] - 20s 29ms/step - loss: 0.1049 - accuracy: 0.7744 - val_loss: 0.0938 - val_accuracy: 0.7749\n",
      "Epoch 3/50\n",
      "697/697 [==============================] - 20s 29ms/step - loss: 0.0783 - accuracy: 0.7780 - val_loss: 0.0945 - val_accuracy: 0.7705\n",
      "Epoch 4/50\n",
      "697/697 [==============================] - 21s 30ms/step - loss: 0.0609 - accuracy: 0.7903 - val_loss: 0.0982 - val_accuracy: 0.7713\n",
      "Epoch 5/50\n",
      "697/697 [==============================] - 24s 35ms/step - loss: 0.0487 - accuracy: 0.7994 - val_loss: 0.0970 - val_accuracy: 0.7713\n",
      "Epoch 6/50\n",
      "697/697 [==============================] - 24s 34ms/step - loss: 0.0419 - accuracy: 0.8040 - val_loss: 0.0997 - val_accuracy: 0.7681\n",
      "Epoch 7/50\n",
      "697/697 [==============================] - 24s 35ms/step - loss: 0.0369 - accuracy: 0.8085 - val_loss: 0.1019 - val_accuracy: 0.7693\n",
      "Epoch 8/50\n",
      "697/697 [==============================] - 24s 35ms/step - loss: 0.0333 - accuracy: 0.8105 - val_loss: 0.1152 - val_accuracy: 0.7608\n",
      "Epoch 9/50\n",
      "697/697 [==============================] - 25s 35ms/step - loss: 0.0304 - accuracy: 0.8127 - val_loss: 0.1114 - val_accuracy: 0.7636\n",
      "Epoch 10/50\n",
      "697/697 [==============================] - 25s 35ms/step - loss: 0.0283 - accuracy: 0.8130 - val_loss: 0.1037 - val_accuracy: 0.7693\n",
      "Epoch 11/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0267 - accuracy: 0.8143 - val_loss: 0.1037 - val_accuracy: 0.7709\n",
      "Epoch 12/50\n",
      "697/697 [==============================] - 25s 35ms/step - loss: 0.0245 - accuracy: 0.8149 - val_loss: 0.1127 - val_accuracy: 0.7660\n",
      "Epoch 13/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0238 - accuracy: 0.8158 - val_loss: 0.1135 - val_accuracy: 0.7664\n",
      "Epoch 14/50\n",
      "697/697 [==============================] - 25s 35ms/step - loss: 0.0227 - accuracy: 0.8166 - val_loss: 0.1156 - val_accuracy: 0.7668\n",
      "Epoch 15/50\n",
      "697/697 [==============================] - 25s 35ms/step - loss: 0.0222 - accuracy: 0.8172 - val_loss: 0.1312 - val_accuracy: 0.7668\n",
      "Epoch 16/50\n",
      "697/697 [==============================] - 25s 35ms/step - loss: 0.0208 - accuracy: 0.8178 - val_loss: 0.1149 - val_accuracy: 0.7664\n",
      "Epoch 17/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0199 - accuracy: 0.8185 - val_loss: 0.1152 - val_accuracy: 0.7681\n",
      "Epoch 18/50\n",
      "697/697 [==============================] - 25s 35ms/step - loss: 0.0191 - accuracy: 0.8189 - val_loss: 0.1238 - val_accuracy: 0.7668\n",
      "Epoch 19/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0186 - accuracy: 0.8188 - val_loss: 0.1165 - val_accuracy: 0.7713\n",
      "Epoch 20/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0197 - accuracy: 0.8181 - val_loss: 0.1204 - val_accuracy: 0.7644\n",
      "Epoch 21/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0179 - accuracy: 0.8193 - val_loss: 0.1207 - val_accuracy: 0.7668\n",
      "Epoch 22/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0177 - accuracy: 0.8189 - val_loss: 0.1284 - val_accuracy: 0.7628\n",
      "Epoch 23/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0177 - accuracy: 0.8191 - val_loss: 0.1210 - val_accuracy: 0.7709\n",
      "Epoch 24/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0184 - accuracy: 0.8188 - val_loss: 0.1152 - val_accuracy: 0.7705\n",
      "Epoch 25/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0176 - accuracy: 0.8194 - val_loss: 0.1201 - val_accuracy: 0.7664\n",
      "Epoch 26/50\n",
      "697/697 [==============================] - 25s 35ms/step - loss: 0.0173 - accuracy: 0.8199 - val_loss: 0.1187 - val_accuracy: 0.7701\n",
      "Epoch 27/50\n",
      "697/697 [==============================] - 25s 35ms/step - loss: 0.0168 - accuracy: 0.8199 - val_loss: 0.1279 - val_accuracy: 0.7660\n",
      "Epoch 28/50\n",
      "697/697 [==============================] - 25s 35ms/step - loss: 0.0165 - accuracy: 0.8204 - val_loss: 0.1221 - val_accuracy: 0.7713\n",
      "Epoch 29/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0165 - accuracy: 0.8204 - val_loss: 0.1342 - val_accuracy: 0.7624\n",
      "Epoch 30/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0162 - accuracy: 0.8206 - val_loss: 0.1348 - val_accuracy: 0.7620\n",
      "Epoch 31/50\n",
      "697/697 [==============================] - 27s 38ms/step - loss: 0.0170 - accuracy: 0.8203 - val_loss: 0.1244 - val_accuracy: 0.7668\n",
      "Epoch 32/50\n",
      "697/697 [==============================] - 24s 35ms/step - loss: 0.0162 - accuracy: 0.8204 - val_loss: 0.1306 - val_accuracy: 0.7652\n",
      "Epoch 33/50\n",
      "697/697 [==============================] - 23s 32ms/step - loss: 0.0164 - accuracy: 0.8205 - val_loss: 0.1267 - val_accuracy: 0.7664\n",
      "Epoch 34/50\n",
      "697/697 [==============================] - 22s 32ms/step - loss: 0.0168 - accuracy: 0.8204 - val_loss: 0.1380 - val_accuracy: 0.7612\n",
      "Epoch 35/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0159 - accuracy: 0.8205 - val_loss: 0.1391 - val_accuracy: 0.7572\n",
      "Epoch 36/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0163 - accuracy: 0.8205 - val_loss: 0.1324 - val_accuracy: 0.7612\n",
      "Epoch 37/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0158 - accuracy: 0.8212 - val_loss: 0.1335 - val_accuracy: 0.7648\n",
      "Epoch 38/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0158 - accuracy: 0.8209 - val_loss: 0.1370 - val_accuracy: 0.7616\n",
      "Epoch 39/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0161 - accuracy: 0.8204 - val_loss: 0.1398 - val_accuracy: 0.7616\n",
      "Epoch 40/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0156 - accuracy: 0.8220 - val_loss: 0.1392 - val_accuracy: 0.7656\n",
      "Epoch 41/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0158 - accuracy: 0.8213 - val_loss: 0.1322 - val_accuracy: 0.7628\n",
      "Epoch 42/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0163 - accuracy: 0.8211 - val_loss: 0.1301 - val_accuracy: 0.7664\n",
      "Epoch 43/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0162 - accuracy: 0.8212 - val_loss: 0.1304 - val_accuracy: 0.7648\n",
      "Epoch 44/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0163 - accuracy: 0.8205 - val_loss: 0.1296 - val_accuracy: 0.7652\n",
      "Epoch 45/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0153 - accuracy: 0.8215 - val_loss: 0.1360 - val_accuracy: 0.7604\n",
      "Epoch 46/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0152 - accuracy: 0.8215 - val_loss: 0.1330 - val_accuracy: 0.7608\n",
      "Epoch 47/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0154 - accuracy: 0.8217 - val_loss: 0.1412 - val_accuracy: 0.7612\n",
      "Epoch 48/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0159 - accuracy: 0.8210 - val_loss: 0.1376 - val_accuracy: 0.7616\n",
      "Epoch 49/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0154 - accuracy: 0.8213 - val_loss: 0.1361 - val_accuracy: 0.7551\n",
      "Epoch 50/50\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.0154 - accuracy: 0.8211 - val_loss: 0.1384 - val_accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=0)\n",
    "hist = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = epochs,\n",
    "                 batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6095cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
