{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "seed_val = 0\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_temp, df_test = train_test_split(dataset, test_size = 0.10, random_state = 0, stratify=dataset['class'])\n",
    "df_train, df_val = train_test_split(df_temp, test_size = 0.10, random_state = 0, stratify=df_temp['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "hidden_dropout_prob = 0.3\n",
    "num_labels = 3\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 1e-2\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(index = df_train[df_train.processed_tweet.isnull()].index, axis=0)\n",
    "df_val = df_val.drop(index = df_val[df_val.processed_tweet.isnull()].index, axis=0)\n",
    "df_test = df_test.drop(index = df_test[df_test.processed_tweet.isnull()].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading bert_base model\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text embedding, including input_ids, token_type_ids and attention_mask\n",
    "def tokenize(df):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "    df.processed_tweet.tolist(),\n",
    "    max_length = 40,\n",
    "    padding='max_length', \n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "train_encoding = tokenize(df_train)\n",
    "val_encoding = tokenize(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = df_train['class'].values\n",
    "val_label = df_val['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_encoding, train_label)\n",
    "val_dataset = NewsDataset(val_encoding, val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define the Bert model\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=num_labels, hidden_dropout_prob=hidden_dropout_prob,output_attentions=False,\n",
    "    output_hidden_states=False )\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer, loss function and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "#optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(train_dataloader)*epochs\n",
    ")\n",
    "\n",
    "total_steps = len(train_dataloader) * 1\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return balanced_accuracy_score(y_true=labels_flat, y_pred=preds_flat)  \n",
    "\n",
    "def plot_confusion_matrix(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    cm = confusion_matrix(labels_flat, preds_flat)\n",
    "    \n",
    "    matrix_proportions = np.zeros((3,3))\n",
    "    for i in range(0,3):\n",
    "        matrix_proportions[i,:] = cm[i,:]/float(cm[i,:].sum())\n",
    "    names=['Hate','Offensive','Neither']\n",
    "    confusion_df = pd.DataFrame(matrix_proportions, index=names,columns=names)\n",
    "    ax = sns.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='YlGnBu',cbar=False, square=True,fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training and validating\n",
    "def train():\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    iter_num = 0\n",
    "    total_iter = len(train_dataloader)\n",
    "    progress_bar = tqdm(train_dataloader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = cross_entropy(outputs[1], labels)\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # update parameter\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "        iter_num += 1\n",
    "        if(iter_num % 100==0):\n",
    "            print(\"epoch: %d, iter_num: %d, loss: %.4f, %.2f%%\" % (epoch, iter_num, loss.item(), iter_num/total_iter*100))\n",
    "        \n",
    "    print(\"Epoch: %d, Average training loss: %.4f\"%(epoch, total_train_loss/len(train_dataloader)))\n",
    "    \n",
    "\n",
    "def validation():\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    predictions_val, true_vals = [], []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        logits = outputs[1]\n",
    "        loss = cross_entropy(outputs[1], labels)\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        predictions_val.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    predictions_val = np.concatenate(predictions_val, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "    \n",
    "    val_f1 = f1_score_func(predictions_val, true_vals)\n",
    "    val_bacc = accuracy(predictions_val, true_vals)\n",
    "    \n",
    "    print(\"Accuracy: %.4f\" % (avg_val_accuracy))\n",
    "    print(\"Average testing loss: %.4f\"%(total_eval_loss/len(val_dataloader)))\n",
    "    print(\"F1 Score (Weighted): %.4f\"%(val_f1))\n",
    "    print(\"Balanced Accuracy Score: %.4f\"%(val_bacc))\n",
    "    print(\"-------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Epoch: 0 ----------------\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter_num: 100, loss: 0.5666, 15.92%\n",
      "epoch: 0, iter_num: 200, loss: 0.6207, 31.85%\n",
      "epoch: 0, iter_num: 300, loss: 0.2678, 47.77%\n",
      "epoch: 0, iter_num: 400, loss: 0.4239, 63.69%\n",
      "epoch: 0, iter_num: 500, loss: 0.3001, 79.62%\n",
      "epoch: 0, iter_num: 600, loss: 0.3232, 95.54%\n",
      "Epoch: 0, Average training loss: 0.4216\n",
      "Accuracy: 0.8978\n",
      "Average testing loss: 0.3097\n",
      "F1 Score (Weighted): 0.8863\n",
      "Balanced Accuracy Score: 0.6657\n",
      "-------------------------------\n",
      "------------Epoch: 1 ----------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter_num: 100, loss: 0.3198, 15.92%\n",
      "epoch: 1, iter_num: 200, loss: 0.4011, 31.85%\n",
      "epoch: 1, iter_num: 300, loss: 0.0939, 47.77%\n",
      "epoch: 1, iter_num: 400, loss: 0.0824, 63.69%\n",
      "epoch: 1, iter_num: 500, loss: 0.2161, 79.62%\n",
      "epoch: 1, iter_num: 600, loss: 0.2494, 95.54%\n",
      "Epoch: 1, Average training loss: 0.2853\n",
      "Accuracy: 0.9043\n",
      "Average testing loss: 0.2975\n",
      "F1 Score (Weighted): 0.8958\n",
      "Balanced Accuracy Score: 0.6922\n",
      "-------------------------------\n",
      "------------Epoch: 2 ----------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, iter_num: 100, loss: 0.3967, 15.92%\n",
      "epoch: 2, iter_num: 200, loss: 0.3437, 31.85%\n",
      "epoch: 2, iter_num: 300, loss: 0.3613, 47.77%\n",
      "epoch: 2, iter_num: 400, loss: 0.1877, 63.69%\n",
      "epoch: 2, iter_num: 500, loss: 0.2304, 79.62%\n",
      "epoch: 2, iter_num: 600, loss: 0.4129, 95.54%\n",
      "Epoch: 2, Average training loss: 0.2593\n",
      "Accuracy: 0.9110\n",
      "Average testing loss: 0.2820\n",
      "F1 Score (Weighted): 0.9073\n",
      "Balanced Accuracy Score: 0.7391\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    print(\"------------Epoch: %d ----------------\" % epoch)\n",
    "    train()\n",
    "    validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement the fine-tuned model on testset\n",
    "test_encoding = tokenize(df_test)\n",
    "test_label = df_test['class'].values\n",
    "test_dataset = NewsDataset(test_encoding, test_label)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    predictions_test, true_test = [], []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        logits = outputs[1]\n",
    "        loss = cross_entropy(outputs[1], labels)\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        predictions_test.append(logits)\n",
    "        true_test.append(label_ids)\n",
    "        \n",
    "    avg_test_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "    predictions_test = np.concatenate(predictions_test, axis=0)\n",
    "    true_test = np.concatenate(true_test, axis=0)\n",
    "    \n",
    "    test_f1 = f1_score_func(predictions_test, true_test)\n",
    "    test_bacc = accuracy(predictions_test, true_test)\n",
    "    \n",
    "    print(\"Accuracy: %.4f\" % (avg_test_accuracy))\n",
    "    print(\"Average testing loss: %.4f\"%(total_eval_loss/len(test_dataloader)))\n",
    "    print(\"F1 Score (Weighted): %.4f\"%(test_f1))\n",
    "    print(\"Balanced Accuracy Score: %.4f\"%(test_bacc))\n",
    "    print(\"-------------------------------\")\n",
    "    \n",
    "    return avg_test_accuracy, predictions_test, true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9013\n",
      "Average testing loss: 0.2779\n",
      "F1 Score (Weighted): 0.8956\n",
      "Balanced Accuracy Score: 0.6957\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "avg_test_accuracy, predictions_test, true_test = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(predictions_test,true_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
